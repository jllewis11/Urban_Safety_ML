{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skorch import NeuralNetClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import folium\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"San_Francisco.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "\n",
    "\n",
    "data[\"Time\"] = pd.to_datetime(data[\"Time\"]).astype(int) / 10**9\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "data[\"Category\"] = encoder.fit_transform(data[\"Category\"])\n",
    "data[\"Part_of_Day\"] = encoder.fit_transform(data[\"Part_of_Day\"])\n",
    "data[\"Day_of_Week\"] = encoder.fit_transform(data[\"Day_of_Week\"])\n",
    "\n",
    "#Drop date\n",
    "data.drop([\"Date\"], axis=1, inplace=True)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "data[[\"Time\", \"Day_of_Week\", \"Part_of_Day\", \"Latitude\", \"Longitude\"]] = scaler.fit_transform(\n",
    "    data[[\"Time\", \"Day_of_Week\", \"Part_of_Day\",\"Latitude\", \"Longitude\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset and dataloader\n",
    "class CrimeDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.data):\n",
    "            raise IndexError\n",
    "        features = torch.tensor(\n",
    "            self.data.loc[idx, ['Time','Day_of_Week','Part_of_Day','Latitude','Longitude']].values, dtype=torch.float\n",
    "        )\n",
    "        label = torch.tensor(self.data.loc[idx, 'Category'], dtype=torch.long)\n",
    "        return features, label\n",
    "\n",
    "\n",
    "dataset = CrimeDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "train_dataset = CrimeDataset(train_data)\n",
    "test_dataset = CrimeDataset(test_data)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Feedforward Neural Network\n",
    "class CrimeNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(CrimeNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.layer3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer3(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "input_size = 5\n",
    "hidden_size = 64\n",
    "num_classes = len(data[\"Category\"].unique())\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = CrimeNet(input_size, hidden_size, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [100/24597], Loss: 1.7730\n",
      "Epoch [1/2], Step [200/24597], Loss: 1.5614\n",
      "Epoch [1/2], Step [300/24597], Loss: 1.6186\n",
      "Epoch [1/2], Step [400/24597], Loss: 1.6372\n",
      "Epoch [1/2], Step [500/24597], Loss: 1.8768\n",
      "Epoch [1/2], Step [600/24597], Loss: 1.6567\n",
      "Epoch [1/2], Step [700/24597], Loss: 1.9221\n",
      "Epoch [1/2], Step [800/24597], Loss: 1.6564\n",
      "Epoch [1/2], Step [900/24597], Loss: 1.5791\n",
      "Epoch [1/2], Step [1000/24597], Loss: 1.7263\n",
      "Epoch [1/2], Step [1100/24597], Loss: 1.7002\n",
      "Epoch [1/2], Step [1200/24597], Loss: 1.7832\n",
      "Epoch [1/2], Step [1300/24597], Loss: 1.7567\n",
      "Epoch [1/2], Step [1400/24597], Loss: 1.7832\n",
      "Epoch [1/2], Step [1500/24597], Loss: 1.6087\n",
      "Epoch [1/2], Step [1600/24597], Loss: 1.7449\n",
      "Epoch [1/2], Step [1700/24597], Loss: 1.4472\n",
      "Epoch [1/2], Step [1800/24597], Loss: 1.7862\n",
      "Epoch [1/2], Step [1900/24597], Loss: 1.6197\n",
      "Epoch [1/2], Step [2000/24597], Loss: 1.7601\n",
      "Epoch [1/2], Step [2100/24597], Loss: 1.5872\n",
      "Epoch [1/2], Step [2200/24597], Loss: 1.6879\n",
      "Epoch [1/2], Step [2300/24597], Loss: 1.6088\n",
      "Epoch [1/2], Step [2400/24597], Loss: 1.6565\n",
      "Epoch [1/2], Step [2500/24597], Loss: 1.6503\n",
      "Epoch [1/2], Step [2600/24597], Loss: 1.7269\n",
      "Epoch [1/2], Step [2700/24597], Loss: 1.6691\n",
      "Epoch [1/2], Step [2800/24597], Loss: 1.4959\n",
      "Epoch [1/2], Step [2900/24597], Loss: 1.6648\n",
      "Epoch [1/2], Step [3000/24597], Loss: 1.6327\n",
      "Epoch [1/2], Step [3100/24597], Loss: 1.6147\n",
      "Epoch [1/2], Step [3200/24597], Loss: 1.6619\n",
      "Epoch [1/2], Step [3300/24597], Loss: 1.5487\n",
      "Epoch [1/2], Step [3400/24597], Loss: 1.8255\n",
      "Epoch [1/2], Step [3500/24597], Loss: 1.7688\n",
      "Epoch [1/2], Step [3600/24597], Loss: 1.6332\n",
      "Epoch [1/2], Step [3700/24597], Loss: 1.6824\n",
      "Epoch [1/2], Step [3800/24597], Loss: 1.7101\n",
      "Epoch [1/2], Step [3900/24597], Loss: 1.6128\n",
      "Epoch [1/2], Step [4000/24597], Loss: 1.5756\n",
      "Epoch [1/2], Step [4100/24597], Loss: 1.4562\n",
      "Epoch [1/2], Step [4200/24597], Loss: 1.5023\n",
      "Epoch [1/2], Step [4300/24597], Loss: 1.7596\n",
      "Epoch [1/2], Step [4400/24597], Loss: 1.8300\n",
      "Epoch [1/2], Step [4500/24597], Loss: 1.5724\n",
      "Epoch [1/2], Step [4600/24597], Loss: 1.7308\n",
      "Epoch [1/2], Step [4700/24597], Loss: 1.6674\n",
      "Epoch [1/2], Step [4800/24597], Loss: 1.6650\n",
      "Epoch [1/2], Step [4900/24597], Loss: 1.8337\n",
      "Epoch [1/2], Step [5000/24597], Loss: 1.5980\n",
      "Epoch [1/2], Step [5100/24597], Loss: 1.7382\n",
      "Epoch [1/2], Step [5200/24597], Loss: 1.7430\n",
      "Epoch [1/2], Step [5300/24597], Loss: 1.6248\n",
      "Epoch [1/2], Step [5400/24597], Loss: 1.7484\n",
      "Epoch [1/2], Step [5500/24597], Loss: 1.7194\n",
      "Epoch [1/2], Step [5600/24597], Loss: 1.6807\n",
      "Epoch [1/2], Step [5700/24597], Loss: 1.6737\n",
      "Epoch [1/2], Step [5800/24597], Loss: 1.7840\n",
      "Epoch [1/2], Step [5900/24597], Loss: 1.6067\n",
      "Epoch [1/2], Step [6000/24597], Loss: 1.7873\n",
      "Epoch [1/2], Step [6100/24597], Loss: 1.4560\n",
      "Epoch [1/2], Step [6200/24597], Loss: 1.6709\n",
      "Epoch [1/2], Step [6300/24597], Loss: 1.7351\n",
      "Epoch [1/2], Step [6400/24597], Loss: 1.3883\n",
      "Epoch [1/2], Step [6500/24597], Loss: 1.7709\n",
      "Epoch [1/2], Step [6600/24597], Loss: 1.5128\n",
      "Epoch [1/2], Step [6700/24597], Loss: 1.5314\n",
      "Epoch [1/2], Step [6800/24597], Loss: 1.5203\n",
      "Epoch [1/2], Step [6900/24597], Loss: 1.7404\n",
      "Epoch [1/2], Step [7000/24597], Loss: 1.5680\n",
      "Epoch [1/2], Step [7100/24597], Loss: 1.6224\n",
      "Epoch [1/2], Step [7200/24597], Loss: 1.8210\n",
      "Epoch [1/2], Step [7300/24597], Loss: 1.8208\n",
      "Epoch [1/2], Step [7400/24597], Loss: 1.9411\n",
      "Epoch [1/2], Step [7500/24597], Loss: 1.5875\n",
      "Epoch [1/2], Step [7600/24597], Loss: 1.6674\n",
      "Epoch [1/2], Step [7700/24597], Loss: 1.4809\n",
      "Epoch [1/2], Step [7800/24597], Loss: 1.7173\n",
      "Epoch [1/2], Step [7900/24597], Loss: 1.4262\n",
      "Epoch [1/2], Step [8000/24597], Loss: 1.8169\n",
      "Epoch [1/2], Step [8100/24597], Loss: 1.6143\n",
      "Epoch [1/2], Step [8200/24597], Loss: 1.6147\n",
      "Epoch [1/2], Step [8300/24597], Loss: 1.5562\n",
      "Epoch [1/2], Step [8400/24597], Loss: 1.7290\n",
      "Epoch [1/2], Step [8500/24597], Loss: 1.9237\n",
      "Epoch [1/2], Step [8600/24597], Loss: 1.5586\n",
      "Epoch [1/2], Step [8700/24597], Loss: 1.5830\n",
      "Epoch [1/2], Step [8800/24597], Loss: 1.7008\n",
      "Epoch [1/2], Step [8900/24597], Loss: 1.5726\n",
      "Epoch [1/2], Step [9000/24597], Loss: 1.6819\n",
      "Epoch [1/2], Step [9100/24597], Loss: 1.5723\n",
      "Epoch [1/2], Step [9200/24597], Loss: 1.7268\n",
      "Epoch [1/2], Step [9300/24597], Loss: 1.5976\n",
      "Epoch [1/2], Step [9400/24597], Loss: 1.7396\n",
      "Epoch [1/2], Step [9500/24597], Loss: 1.5982\n",
      "Epoch [1/2], Step [9600/24597], Loss: 1.5944\n",
      "Epoch [1/2], Step [9700/24597], Loss: 1.6453\n",
      "Epoch [1/2], Step [9800/24597], Loss: 1.6506\n",
      "Epoch [1/2], Step [9900/24597], Loss: 1.6858\n",
      "Epoch [1/2], Step [10000/24597], Loss: 1.7420\n",
      "Epoch [1/2], Step [10100/24597], Loss: 1.4632\n",
      "Epoch [1/2], Step [10200/24597], Loss: 1.6154\n",
      "Epoch [1/2], Step [10300/24597], Loss: 1.6292\n",
      "Epoch [1/2], Step [10400/24597], Loss: 1.4806\n",
      "Epoch [1/2], Step [10500/24597], Loss: 1.6923\n",
      "Epoch [1/2], Step [10600/24597], Loss: 1.7111\n",
      "Epoch [1/2], Step [10700/24597], Loss: 1.7347\n",
      "Epoch [1/2], Step [10800/24597], Loss: 1.6570\n",
      "Epoch [1/2], Step [10900/24597], Loss: 1.6729\n",
      "Epoch [1/2], Step [11000/24597], Loss: 1.5829\n",
      "Epoch [1/2], Step [11100/24597], Loss: 1.6492\n",
      "Epoch [1/2], Step [11200/24597], Loss: 1.6943\n",
      "Epoch [1/2], Step [11300/24597], Loss: 1.6221\n",
      "Epoch [1/2], Step [11400/24597], Loss: 1.5520\n",
      "Epoch [1/2], Step [11500/24597], Loss: 1.6545\n",
      "Epoch [1/2], Step [11600/24597], Loss: 1.6215\n",
      "Epoch [1/2], Step [11700/24597], Loss: 1.5537\n",
      "Epoch [1/2], Step [11800/24597], Loss: 1.7638\n",
      "Epoch [1/2], Step [11900/24597], Loss: 1.5674\n",
      "Epoch [1/2], Step [12000/24597], Loss: 1.7428\n",
      "Epoch [1/2], Step [12100/24597], Loss: 1.5017\n",
      "Epoch [1/2], Step [12200/24597], Loss: 1.6761\n",
      "Epoch [1/2], Step [12300/24597], Loss: 1.6325\n",
      "Epoch [1/2], Step [12400/24597], Loss: 1.6954\n",
      "Epoch [1/2], Step [12500/24597], Loss: 1.6779\n",
      "Epoch [1/2], Step [12600/24597], Loss: 1.5096\n",
      "Epoch [1/2], Step [12700/24597], Loss: 1.5931\n",
      "Epoch [1/2], Step [12800/24597], Loss: 1.5446\n",
      "Epoch [1/2], Step [12900/24597], Loss: 1.8149\n",
      "Epoch [1/2], Step [13000/24597], Loss: 1.4651\n",
      "Epoch [1/2], Step [13100/24597], Loss: 1.7143\n",
      "Epoch [1/2], Step [13200/24597], Loss: 1.7136\n",
      "Epoch [1/2], Step [13300/24597], Loss: 1.5664\n",
      "Epoch [1/2], Step [13400/24597], Loss: 1.7381\n",
      "Epoch [1/2], Step [13500/24597], Loss: 1.6154\n",
      "Epoch [1/2], Step [13600/24597], Loss: 1.7855\n",
      "Epoch [1/2], Step [13700/24597], Loss: 1.8618\n",
      "Epoch [1/2], Step [13800/24597], Loss: 1.6186\n",
      "Epoch [1/2], Step [13900/24597], Loss: 1.6907\n",
      "Epoch [1/2], Step [14000/24597], Loss: 1.6813\n",
      "Epoch [1/2], Step [14100/24597], Loss: 1.6402\n",
      "Epoch [1/2], Step [14200/24597], Loss: 1.4865\n",
      "Epoch [1/2], Step [14300/24597], Loss: 1.4902\n",
      "Epoch [1/2], Step [14400/24597], Loss: 1.8447\n",
      "Epoch [1/2], Step [14500/24597], Loss: 1.7159\n",
      "Epoch [1/2], Step [14600/24597], Loss: 1.6201\n",
      "Epoch [1/2], Step [14700/24597], Loss: 1.6436\n",
      "Epoch [1/2], Step [14800/24597], Loss: 1.5877\n",
      "Epoch [1/2], Step [14900/24597], Loss: 1.6464\n",
      "Epoch [1/2], Step [15000/24597], Loss: 1.5332\n",
      "Epoch [1/2], Step [15100/24597], Loss: 1.8088\n",
      "Epoch [1/2], Step [15200/24597], Loss: 1.7204\n",
      "Epoch [1/2], Step [15300/24597], Loss: 1.9350\n",
      "Epoch [1/2], Step [15400/24597], Loss: 1.7706\n",
      "Epoch [1/2], Step [15500/24597], Loss: 1.8120\n",
      "Epoch [1/2], Step [15600/24597], Loss: 1.5278\n",
      "Epoch [1/2], Step [15700/24597], Loss: 1.7157\n",
      "Epoch [1/2], Step [15800/24597], Loss: 1.6351\n",
      "Epoch [1/2], Step [15900/24597], Loss: 1.7014\n",
      "Epoch [1/2], Step [16000/24597], Loss: 1.5686\n",
      "Epoch [1/2], Step [16100/24597], Loss: 1.5082\n",
      "Epoch [1/2], Step [16200/24597], Loss: 1.8167\n",
      "Epoch [1/2], Step [16300/24597], Loss: 1.6479\n",
      "Epoch [1/2], Step [16400/24597], Loss: 1.7071\n",
      "Epoch [1/2], Step [16500/24597], Loss: 1.6005\n",
      "Epoch [1/2], Step [16600/24597], Loss: 1.4704\n",
      "Epoch [1/2], Step [16700/24597], Loss: 1.5721\n",
      "Epoch [1/2], Step [16800/24597], Loss: 1.7104\n",
      "Epoch [1/2], Step [16900/24597], Loss: 1.6815\n",
      "Epoch [1/2], Step [17000/24597], Loss: 1.7265\n",
      "Epoch [1/2], Step [17100/24597], Loss: 1.7118\n",
      "Epoch [1/2], Step [17200/24597], Loss: 1.6537\n",
      "Epoch [1/2], Step [17300/24597], Loss: 1.7001\n",
      "Epoch [1/2], Step [17400/24597], Loss: 1.7240\n",
      "Epoch [1/2], Step [17500/24597], Loss: 1.9939\n",
      "Epoch [1/2], Step [17600/24597], Loss: 1.7873\n",
      "Epoch [1/2], Step [17700/24597], Loss: 1.7552\n",
      "Epoch [1/2], Step [17800/24597], Loss: 1.4684\n",
      "Epoch [1/2], Step [17900/24597], Loss: 1.6383\n",
      "Epoch [1/2], Step [18000/24597], Loss: 1.6363\n",
      "Epoch [1/2], Step [18100/24597], Loss: 1.5164\n",
      "Epoch [1/2], Step [18200/24597], Loss: 1.5715\n",
      "Epoch [1/2], Step [18300/24597], Loss: 1.6941\n",
      "Epoch [1/2], Step [18400/24597], Loss: 1.6586\n",
      "Epoch [1/2], Step [18500/24597], Loss: 1.7215\n",
      "Epoch [1/2], Step [18600/24597], Loss: 1.6726\n",
      "Epoch [1/2], Step [18700/24597], Loss: 1.6992\n",
      "Epoch [1/2], Step [18800/24597], Loss: 1.7069\n",
      "Epoch [1/2], Step [18900/24597], Loss: 1.7476\n",
      "Epoch [1/2], Step [19000/24597], Loss: 1.7667\n",
      "Epoch [1/2], Step [19100/24597], Loss: 1.4886\n",
      "Epoch [1/2], Step [19200/24597], Loss: 1.7056\n",
      "Epoch [1/2], Step [19300/24597], Loss: 1.8347\n",
      "Epoch [1/2], Step [19400/24597], Loss: 1.7784\n",
      "Epoch [1/2], Step [19500/24597], Loss: 1.4792\n",
      "Epoch [1/2], Step [19600/24597], Loss: 1.5831\n",
      "Epoch [1/2], Step [19700/24597], Loss: 1.5848\n",
      "Epoch [1/2], Step [19800/24597], Loss: 1.9316\n",
      "Epoch [1/2], Step [19900/24597], Loss: 1.6072\n",
      "Epoch [1/2], Step [20000/24597], Loss: 1.6239\n",
      "Epoch [1/2], Step [20100/24597], Loss: 1.4934\n",
      "Epoch [1/2], Step [20200/24597], Loss: 1.6390\n",
      "Epoch [1/2], Step [20300/24597], Loss: 1.4850\n",
      "Epoch [1/2], Step [20400/24597], Loss: 1.6281\n",
      "Epoch [1/2], Step [20500/24597], Loss: 1.5110\n",
      "Epoch [1/2], Step [20600/24597], Loss: 1.5607\n",
      "Epoch [1/2], Step [20700/24597], Loss: 1.6802\n",
      "Epoch [1/2], Step [20800/24597], Loss: 1.7787\n",
      "Epoch [1/2], Step [20900/24597], Loss: 1.8299\n",
      "Epoch [1/2], Step [21000/24597], Loss: 1.4665\n",
      "Epoch [1/2], Step [21100/24597], Loss: 1.5876\n",
      "Epoch [1/2], Step [21200/24597], Loss: 1.8469\n",
      "Epoch [1/2], Step [21300/24597], Loss: 1.5113\n",
      "Epoch [1/2], Step [21400/24597], Loss: 1.8770\n",
      "Epoch [1/2], Step [21500/24597], Loss: 1.4936\n",
      "Epoch [1/2], Step [21600/24597], Loss: 1.5710\n",
      "Epoch [1/2], Step [21700/24597], Loss: 1.7304\n",
      "Epoch [1/2], Step [21800/24597], Loss: 1.5468\n",
      "Epoch [1/2], Step [21900/24597], Loss: 1.7565\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (features, labels) in enumerate(dataloader):\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(dataloader)}], Loss: {loss.item():.4f}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_dataloader:\n",
    "        features = features.unsqueeze(1)\n",
    "        outputs = model(features)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_predictions.extend(predicted.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy, confusion matrix, and classification report\n",
    "all_labels_np = np.array(all_labels)\n",
    "all_predictions_np = np.array(all_predictions)\n",
    "accuracy = accuracy_score(all_labels_np, all_predictions_np)\n",
    "\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "class_report = classification_report(all_labels, all_predictions)\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", class_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
